# ft_linear_regression

A from-scratch implementation of linear regression using gradient descent to predict car prices based on mileage.

## ğŸ“‹ Project Overview

This project implements a simple machine learning algorithm to predict car prices using linear regression trained with gradient descent. The implementation follows the mathematical foundations of ML without relying on pre-built regression libraries.

### Key Features
- âœ… Manual gradient descent implementation
- âœ… Feature scaling for numerical stability
- âœ… Two-program architecture (training + prediction)
- âœ… Comprehensive visualization and analysis
- âœ… Performance metrics (MSE, MAE, RÂ²)
- âœ… Convergence tracking

---

## ğŸ¯ Learning Objectives

- Understand gradient descent optimization
- Implement linear regression from scratch
- Learn feature scaling techniques
- Visualize model performance
- Evaluate regression metrics

---

## ğŸ“‚ Project Structure
```
ft_linear_regression/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ data.csv              # Training dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                  # Training program (mandatory)
â”‚   â””â”€â”€ predict.py                # Prediction program (mandatory)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb         # Visualization & analysis (bonus)
â”œâ”€â”€ thetas.txt                    # Saved model parameters
â”œâ”€â”€ error_history.txt             # Training convergence data
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.7+
- pip (Python package manager)

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd ft_linear_regression
```

2. **Create virtual environment (recommended)**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

---

## ğŸ’» Usage

### 1. Train the Model

Run the training program to learn the optimal parameters (Î¸â‚€ and Î¸â‚):
```bash
python src/train.py
```

**Output:**
```
Training completed
theta0: 6770.871133537634
theta1: -644.5311741323535
```

**What it does:**
- Reads the dataset from `data/raw/data.csv`
- Initializes Î¸â‚€ = 0, Î¸â‚ = 0
- Runs gradient descent for 1000 iterations
- Saves learned parameters to `thetas.txt`
- Saves training history to `error_history.txt`

---

### 2. Predict Car Prices

Run the prediction program to estimate prices for specific mileage:
```bash
python src/predict.py
```

**Example interaction:**
```
Enter car mileage (km): 150000
Estimated price: 5804.21
```

**What it does:**
- Loads trained parameters from `thetas.txt`
- Prompts user for mileage input
- Applies the learned model: `price = Î¸â‚€ + Î¸â‚ Ã— (mileage / 100,000)`
- Returns predicted price

---

### 3. View Analysis & Visualizations (Bonus)

Open the Jupyter notebook to see comprehensive analysis:
```bash
jupyter notebook notebooks/exploration.ipynb
```

**Notebook includes:**
- âœ… **Bonus 1:** Scatter plot of dataset
- âœ… **Bonus 2:** Regression line visualization
- âœ… **Bonus 3:** Performance metrics (MSE, MAE, RÂ²)
- ğŸ“Š Gradient descent convergence plot
- ğŸ“ˆ Comprehensive model summary

---

## ğŸ§® Implementation Details

### Algorithm: Gradient Descent

The model uses batch gradient descent to minimize the cost function:

**Hypothesis:**
```
estimatePrice(mileage) = Î¸â‚€ + Î¸â‚ Ã— mileage
```

**Update Rules:**
```
tmpÎ¸â‚€ = learningRate Ã— (1/m) Ã— Î£(estimatePrice - actualPrice)
tmpÎ¸â‚ = learningRate Ã— (1/m) Ã— Î£((estimatePrice - actualPrice) Ã— mileage)

Î¸â‚€ = Î¸â‚€ - tmpÎ¸â‚€
Î¸â‚ = Î¸â‚ - tmpÎ¸â‚
```

Where:
- `m` = number of training samples
- `learningRate` = 0.01
- `Î£` = sum over all samples

### Feature Scaling

**Why:** Raw mileage values (up to 240,000) caused numerical overflow during gradient descent.

**Solution:** Divide all mileage values by 100,000

**Formula:**
```python
km_scaled = km / 100000
```

This is applied consistently in both training and prediction.

### Model Parameters

After training on the provided dataset:

| Parameter | Value | Interpretation |
|-----------|-------|----------------|
| Î¸â‚€ (intercept) | 6,770.87 | Base price (extrapolated at 0 km) |
| Î¸â‚ (slope) | -644.53 | Price reduction per 100,000 km |

**Meaning:** For every 100,000 km of mileage, the predicted price decreases by approximately 645 units.

---

## ğŸ“Š Model Performance

### Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **RÂ² Score** | 0.3473 | Mileage explains 34.73% of price variation |
| **MAE** | 835.19 | Average prediction error: Â±835 |
| **MSE** | 1,089,303.75 | Mean squared error |
| **Error Reduction** | 97.4% | Training improved from initial random state |

### Interpretation

**RÂ² = 0.35** indicates that mileage alone explains about 35% of car price variation. The remaining 65% is influenced by other factors not in the dataset:
- Brand/manufacturer
- Model year
- Vehicle condition
- Features & equipment
- Market location

For a **single-feature linear model**, this represents a **moderate and significant relationship**.

---

## ğŸ› ï¸ Technical Specifications

### Dependencies
```
pandas>=2.0.0       # Data loading and manipulation
numpy>=1.24.0       # Numerical operations
matplotlib>=3.7.0   # Visualization (bonus only)
```

### Allowed Libraries

âœ… **Permitted:**
- `pandas` - CSV reading and data handling
- `numpy` - Array operations and mathematical functions
- `matplotlib` - Plotting and visualization

âŒ **Forbidden:**
- `sklearn.linear_model` - Automatic regression (defeats learning purpose)
- `numpy.polyfit` - Polynomial fitting (computes parameters automatically)
- Any library that computes Î¸ values without manual gradient descent

---

## ğŸ“ What This Project Demonstrates

### Mandatory Requirements âœ…

1. **Two separate programs:**
   - `train.py` - Trains the model using gradient descent
   - `predict.py` - Predicts prices using trained parameters

2. **Correct implementation:**
   - Exact hypothesis formula as specified
   - Exact gradient descent formulas as specified
   - Simultaneous theta updates (using temporary variables)

3. **Model persistence:**
   - Parameters saved to file after training
   - Parameters loaded from file for prediction

### Bonus Requirements âœ…

1. **Data visualization** - Scatter plot showing mileage vs price distribution
2. **Regression line plot** - Learned model overlaid on data
3. **Precision calculation** - Multiple metrics (MSE, MAE, RÂ²)

### Additional Analysis ğŸŒŸ

- Gradient descent convergence visualization
- Training error tracking across iterations
- Comprehensive model validation
- Professional documentation

---

## ğŸ§ª Testing the Implementation

### Verify Training Works
```bash
# Remove old parameters (if they exist)
rm -f thetas.txt error_history.txt

# Train from scratch
python src/train.py

# Check that files were created
ls thetas.txt error_history.txt
```

### Verify Prediction Works
```bash
# Test with known mileage
echo "100000" | python src/predict.py
# Expected output: ~6,126
```

### Verify Scaling Consistency

Both programs use the same scaling factor (Ã·100,000), ensuring predictions match the trained model.

---

## ğŸ“ Design Decisions

### Why Feature Scaling?

**Problem:** Without scaling, gradient descent experienced numerical overflow:
```
km = 240,000
error Ã— km = huge number â†’ NaN
```

**Solution:** Scale down by 100,000:
```
km_scaled = 2.4
error Ã— km_scaled = manageable â†’ stable training
```

### Why Separate Programs?

**Architecture principle:** Separation of concerns
- Training is a one-time batch operation
- Prediction is an interactive, repeated operation
- Keeps each program focused and testable

### Why Learning Rate = 0.01?

**Balance:**
- Too high (e.g., 0.1) â†’ Unstable, diverges
- Too low (e.g., 0.0001) â†’ Converges very slowly
- 0.01 â†’ Good balance: stable and reasonably fast

---

## ğŸ› Troubleshooting

### Error: "thetas.txt not found"

**Cause:** Prediction program cannot find trained parameters

**Solution:** Run training first:
```bash
python src/train.py
```

### Error: "data.csv not found"

**Cause:** Dataset not in expected location

**Solution:** Ensure dataset is at `data/raw/data.csv`

### Training produces NaN values

**Cause:** Feature scaling not applied or learning rate too high

**Solution:** Current implementation already handles this correctly with scaling

---

## ğŸ” Code Quality

### Best Practices Followed

- âœ… Clear function names and variable names
- âœ… Explanatory comments for complex operations
- âœ… Error handling for user inputs
- âœ… Modular design (functions over monolithic code)
- âœ… No magic numbers (constants are named)
- âœ… Consistent formatting and style

### Compliance

- âœ… No forbidden libraries
- âœ… Manual gradient descent implementation
- âœ… Exact formulas as specified in subject
- âœ… Proper simultaneous parameter updates

---

## ğŸ“š Learning Resources

To understand this implementation better:

- **Gradient Descent:** Andrew Ng's Machine Learning Course (Coursera)
- **Linear Regression:** "Introduction to Statistical Learning" (Chapter 3)
- **Python ML:** "Hands-On Machine Learning" by AurÃ©lien GÃ©ron

---

- Implementation: January 2026
- Project: ft_linear_regression (Curriculum)

---

## âœ… Project Status

**Status:** Completed  âœ…

**All mandatory requirements:** âœ… Implemented and tested
**All bonus requirements:** âœ… Implemented and documented
**Code quality:** âœ… Clean, commented, and professional

---